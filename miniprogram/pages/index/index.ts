import AliSpeechRecognizer from '../../utils/customSpeechRecognition'; // Rename old import
import DashScopeSpeechRecognizer from '../../utils/dashScopeSpeechRecognition'; // Import new one
import { DEEPSEEK_SECRETS } from '../../utils/secrets';
import { synthesizeAndPlay, stopPlayback as stopTTSPlayback, setTtsProvider, getCurrentTtsProvider, TtsProviderType } from '../../utils/ttsProvider'; // Import TTS functions from the provider
import { callQwenApi, callQwenApiWithHistory } from '../../utils/qwenApi';
import { AI_INITIAL_PROMPT, FUNCTION_PATTERN, BUTTON_PATTERN, RECORD_PATTERN, FUNCTION_ROUTES, FunctionName } from '../../utils/config'; // Import AI config
import { FEATURE_METADATA } from '../../utils/featureMetadata'; // Import feature metadata

// Define ASR engine types
type AsrEngineType = 'alibaba' | 'dashscope';

// Define the possible states for the orb animation
type OrbState = 'idle' | 'listening' | 'listening-active' | 'processing' | 'speaking'; // Add speaking state

// Define the button interface
interface Button {
    id: number;
    feature: string;
    name: string;
    description?: string;
    image?: string;
}

interface ChatMessage {
    id: number;
    role: 'user' | 'assistant';
    content: string;
    hint?: string;
    buttons?: Button[]; // Add buttons field
}

interface IPageData {
    debugRecognizedText: string;    // For Debug Box 1
    debugDeepseekResponse: string;  // For Debug Box 2
    isRecording: boolean;           // Is microphone active?
    isWaitingForDeepseek: boolean;  // Waiting for API response?
    isSpeaking: boolean;            // Is TTS currently playing?
    orbState: OrbState;             // Controls orb animation
    currentAsrEngine: AsrEngineType; // Add state for current engine
    currentAiModel: 'deepseek' | 'qwen'; // Add new property for AI model selection
    currentTtsProvider: TtsProviderType; // Add property for current TTS provider
    chatHistory: ChatMessage[];
    lastMessageId: string;
    isEditing: boolean;             // Is the text input being edited?
    accumulatedText: string;        // Accumulated recognized text during recording
    showSettings: boolean;          // Controls the visibility of settings menu
    isDebugMode: boolean;           // Controls debug mode to prevent API requests
}

// Helper function to play text with TTS
function playTextWithTTS(
    this: WechatMiniprogram.Page.Instance<IPageData, WechatMiniprogram.IAnyObject>,
    text: string,
    shouldAutoRecord: boolean,
    logId: string = ''
): void {
    if (!text.trim()) {
        this.setData({ orbState: 'idle' });
        return;
    }

    console.log(`[TTS] Attempting to play response: ${logId}`, text);
    this.setData({ isSpeaking: true, orbState: 'speaking' });

    // Ê†πÊçÆÂΩìÂâçÁöÑTTSÊèê‰æõÂïÜÈÄâÊã©‰∏çÂêåÁöÑËØ≠Èü≥ÂíåÈááÊ†∑Áéá
    let voice, sampleRate;
    if (this.data.currentTtsProvider === 'cosyvoice') {
        // cosyvoice‰ΩøÁî®ÂÆòÊñπÁ§∫‰æã‰∏≠Êé®ËçêÁöÑÂèÇÊï∞
        voice = 'longwan';
        sampleRate = 22050;
    } else {
        // ÈòøÈáå‰∫ë‰ΩøÁî®ÈªòËÆ§ÁöÑÂèÇÊï∞
        voice = 'aitong';
        sampleRate = 16000;
    }

    synthesizeAndPlay(
        text,
        voice,
        'mp3',
        sampleRate,
        () => {
            console.log("[TTS] Playback completed successfully.");
            // If shouldAutoRecord is true, start recording after speech completes
            if (shouldAutoRecord) {
                console.log("[Auto-Record] Starting recording automatically after speech");
                this.startRecordingAndRecognition();
            } else {
                this.setData({ isSpeaking: false, orbState: 'idle' });
            }
        },
        (error) => {
            console.error("[TTS] Playback failed:", error);
            this.setData({ isSpeaking: false, orbState: 'idle' });
            // Show a toast for TTS playback error only if it's not "Close received after close"
            // since that error often happens when switching screens or interrupting playback
            if (error && error.errMsg &&
                !error.errMsg.includes("Close received after close") &&
                !error.errMsg.includes("Êú™ÂÆåÊàêÁöÑÊìç‰Ωú")) {
                wx.showToast({
                    title: 'ËØ≠Èü≥Êí≠ÊîæÂ§±Ë¥•ÔºåÂ∞Ü‰ΩøÁî®Â§áÁî®ÂºïÊìé',
                    icon: 'none'
                });
            }
        }
    );
}

// Choose the default ASR engine here
const DEFAULT_ASR_ENGINE: AsrEngineType = 'dashscope'; // Or 'alibaba'

// Define a common interface for our speech recognizers
interface ISpeechRecognizer {
    onResult(callback: (text: string, isFinal: boolean) => void): void;
    onError(callback: (error: any) => void): void;
    start(): Promise<void>;
    stop(): Promise<void>;
    reset(): void;
}

// Add a function to check for and handle special function triggers in AI responses
function checkAndHandleFunctionTriggers(text: string): { processedText: string, functionFound: boolean, buttons: Button[], shouldAutoRecord: boolean } {
    // Don't modify the original text that is displayed to the user
    let processedText = text;
    let functionFound = false;
    let buttons: Button[] = [];
    let shouldAutoRecord = false;

    // Check for goto function pattern
    const match = text.match(FUNCTION_PATTERN);
    if (match && match[1]) {
        const functionName = match[1] as FunctionName;
        console.log(`Function detected: ${functionName}`);

        // Âú®‰ªª‰ΩïÂØºËà™ÂâçÂÅúÊ≠¢TTSÊí≠Êîæ
        stopTTSPlayback(true); // trueÂèÇÊï∞ÊäëÂà∂ÈîôËØØÊèêÁ§∫

        // Special handling for hongbao function
        if (functionName === 'hongbao') {
            // Navigate directly to event-demo with redpacket_tutorial ID
            wx.navigateTo({
                url: '/pages/event-demo/event-demo?id=redpacket_tutorial',
                success: () => {
                    console.log('Successfully navigated to hongbao tutorial');
                },
                fail: (err) => {
                    console.error('Navigation to hongbao failed:', err);
                    wx.showToast({
                        title: 'Ë∑≥ËΩ¨Â§±Ë¥•',
                        icon: 'none'
                    });
                }
            });
            functionFound = true;
        }
        // For all other functions, use the routes map
        else if (FUNCTION_ROUTES[functionName]) {
            // Navigate to the appropriate page
            wx.navigateTo({
                url: FUNCTION_ROUTES[functionName],
                success: () => {
                    console.log(`Successfully navigated to function: ${functionName}`);
                },
                fail: (err) => {
                    console.error('Navigation failed:', err);
                    wx.showToast({
                        title: 'ÂäüËÉΩË∑≥ËΩ¨Â§±Ë¥•',
                        icon: 'none'
                    });
                }
            });
            functionFound = true;
        }

        // Remove the function tag from the displayed text
        processedText = text.replace(FUNCTION_PATTERN, '').trim();
    }

    // Check for record tag
    if (text.match(RECORD_PATTERN)) {
        console.log('Record command detected in AI response');
        shouldAutoRecord = true;
        processedText = processedText.replace(RECORD_PATTERN, '').trim();
    }

    // Check for button patterns (can have multiple in one message)
    let buttonMatch;
    // Reset lastIndex to avoid infinite loop
    BUTTON_PATTERN.lastIndex = 0;
    while ((buttonMatch = BUTTON_PATTERN.exec(text)) !== null) {
        const featureName = buttonMatch[1];

        if (FEATURE_METADATA[featureName]) {
            const metadata = FEATURE_METADATA[featureName];
            buttons.push({
                id: Date.now() + buttons.length, // Unique ID for each button
                feature: featureName,
                name: metadata.name,
                description: metadata.description,
                image: metadata.image
            });
        }
    }

    // Remove all button tags from the text
    processedText = processedText.replace(BUTTON_PATTERN, '').trim();

    // Limit buttons to maximum 3
    if (buttons.length > 3) {
        buttons = buttons.slice(0, 3);
    }

    return { processedText, functionFound, buttons, shouldAutoRecord };
}

Page<IPageData, WechatMiniprogram.IAnyObject>({
    data: {
        debugRecognizedText: '',
        debugDeepseekResponse: '',
        isRecording: false,
        isWaitingForDeepseek: false,
        isSpeaking: false,
        orbState: 'idle',
        currentAsrEngine: DEFAULT_ASR_ENGINE, // Initialize with default
        currentAiModel: 'qwen', // Default to qwen instead of deepseek
        currentTtsProvider: 'cosyvoice', // Default to Alibaba Cloud TTS
        chatHistory: [],
        lastMessageId: '',
        isEditing: false,         // Add isEditing state
        accumulatedText: '',      // Accumulated recognized text during recording
        showSettings: false,      // Add showSettings state for settings menu
        isDebugMode: true,        // Start with debug mode enabled by default
    },

    // Add a property to maintain the actual conversation history for API calls
    // This is separate from chatHistory which is for display purposes
    conversationHistory: [] as { role: 'system' | 'user' | 'assistant', content: string }[],

    // Silence detection timer
    silenceTimer: null as number | null,
    // Activity timeout (for bouncing animation)
    activityTimer: null as number | null,
    // Store the latest recognized text before sending
    latestRecognizedText: '',
    // Last result timestamp to detect activity
    lastResultTime: 0,

    // Speech recognizer instance - now typed with the interface
    speechRecognizer: null as ISpeechRecognizer | null,

    // Add a flag to prevent rapid duplicate error handling
    lastErrorTime: 0,
    minErrorInterval: 500, // Minimum interval in ms between handling errors

    // Handle feature button tap
    handleFeatureButtonTap: function (e: WechatMiniprogram.CustomEvent) {
        const feature = e.currentTarget.dataset.feature;
        console.log('Feature button tapped:', feature);

        // Âú®ÂØºËà™ÂâçÂÅúÊ≠¢ËØ≠Èü≥Êí≠Êîæ
        stopTTSPlayback(true); // trueÂèÇÊï∞ÊäëÂà∂ÈîôËØØÊèêÁ§∫
        this.setData({
            isSpeaking: false,
            orbState: 'idle'
        });

        if (feature && FUNCTION_ROUTES[feature as FunctionName]) {
            wx.navigateTo({
                url: FUNCTION_ROUTES[feature as FunctionName],
                success: () => {
                    console.log(`Successfully navigated to feature: ${feature}`);
                },
                fail: (err) => {
                    console.error('Navigation failed:', err);
                    wx.showToast({
                        title: 'ÂäüËÉΩË∑≥ËΩ¨Â§±Ë¥•',
                        icon: 'none'
                    });
                }
            });
        }
    },

    onLoad: function () {
        this.initSpeechRecognition(); // Will use the engine specified in data

        // Only send initial prompt if not in debug mode
        if (!this.data.isDebugMode) {
            this.sendInitialPromptToAI();
        } else {
            console.log('[Debug] Skipping initial API request in debug mode');

            // Ëá™ÂÆö‰πâÁöÑ Debug Ê®°ÂºèÈóÆÂÄôËØ≠ÔºåÂåÖÂê´ÊåâÈíÆÂíåËÆ∞ÂΩïÊ†áÁ≠æ
            const debugWelcomeText = "Debug: ÊÇ®Â•ΩÂëÄÔºåÊúâ‰ªÄ‰πàÊàëÂèØ‰ª•Â∏ÆÊÇ®ÁöÑÂêóÔºü[button:hongbao] [button:health] [button:emergency] [record]";

            // Â§ÑÁêÜÊñáÊú¨‰∏≠ÁöÑÊ†áÁ≠æÔºåÊèêÂèñÊåâÈíÆÂíåÂÖ∂‰ªñÂäüËÉΩ
            const { processedText, functionFound, buttons, shouldAutoRecord } = checkAndHandleFunctionTriggers(debugWelcomeText);

            // ÂàõÂª∫ÁªèËøáÂ§ÑÁêÜÁöÑË∞ÉËØïÊ∂àÊÅØ
            const debugMessage: ChatMessage = {
                id: Date.now(),
                role: 'assistant',
                content: processedText,
                buttons: buttons.length > 0 ? buttons : undefined,
                hint: shouldAutoRecord ? '(Á≠âÂæÖÊÇ®ÁöÑËØ≠Èü≥ÂõûÂ§ç)' : undefined
            };

            this.setData({
                chatHistory: [debugMessage]
            });
        }
    },

    onHide: function () {
        // ÂΩìÈ°µÈù¢ÈöêËóèÔºàÂåÖÊã¨Ë∑≥ËΩ¨Âà∞ÂÖ∂‰ªñÈ°µÈù¢ÔºâÊó∂ÂÅúÊ≠¢ËØ≠Èü≥Êí≠Êîæ
        console.log('Index page hidden, stopping TTS playback');
        stopTTSPlayback(true); // ‰ΩøÁî®trueÂèÇÊï∞ÊäëÂà∂ÈîôËØØÊèêÁ§∫

        // Êõ¥Êñ∞UIÁä∂ÊÄÅ
        this.setData({
            isSpeaking: false,
            orbState: 'idle'
        });
    },

    onUnload: function () {
        // Clean up resources
        if (this.speechRecognizer) {
            if (typeof this.speechRecognizer.stop === 'function') {
                this.speechRecognizer.stop();
            }
        }
        stopTTSPlayback(); // Ensure TTS stops if page is closed
        if (this.silenceTimer) {
            clearTimeout(this.silenceTimer);
            this.silenceTimer = null;
        }
        if (this.activityTimer) {
            clearTimeout(this.activityTimer);
            this.activityTimer = null;
        }
    },

    // Initialize Speech Recognition based on currentAsrEngine
    initSpeechRecognition: function () {
        const engineType = this.data.currentAsrEngine;
        console.log(`Initializing ASR engine: ${engineType}`);

        // Instantiate the selected recognizer
        if (engineType === 'dashscope') {
            this.speechRecognizer = new DashScopeSpeechRecognizer();
        } else { // Default to Alibaba
            this.speechRecognizer = new AliSpeechRecognizer();
        }

        if (!this.speechRecognizer) {
            console.error("Failed to initialize speech recognizer!");
            wx.showToast({ title: 'ËØ≠Èü≥ÂºïÊìéÂàùÂßãÂåñÂ§±Ë¥•', icon: 'none' });
            return;
        }

        // --- Result Callback ---
        this.speechRecognizer.onResult((text: string, isFinal: boolean) => {
            console.log('ËØ≠Èü≥ËØÜÂà´ÁªìÊûú:', text, `isFinal: ${isFinal}`);

            // Detect voice activity - received a result means voice was detected
            this.handleVoiceActivity();

            // Â¶ÇÊûúÊòØÁ©∫ÊñáÊú¨Ôºå‰∏çÊõ¥Êñ∞ÊòæÁ§∫
            if (!text.trim()) {
                return;
            }

            // Á¥ØÁßØÂ∑≤ËØÜÂà´ÁöÑÊñáÊú¨ÔºåÂ∞ÜÊñ∞ÊñáÊú¨ËøΩÂä†Âà∞Â∑≤Á¥ØÁßØÁöÑÊñáÊú¨‰∏≠
            // Â¶ÇÊûúÊñ∞ÊñáÊú¨ÊòØÂ∑≤Á¥ØÁßØÊñáÊú¨ÁöÑ‰∏ÄÈÉ®ÂàÜÔºàËØÜÂà´ÂºïÊìéÂèØËÉΩËøîÂõûÈÉ®ÂàÜÁªìÊûúÔºâÔºåÂàô‰ΩøÁî®Êõ¥ÈïøÁöÑÈÇ£‰∏™
            let updatedText = text;
            const currentAccumulated = this.data.accumulatedText;

            // Ê£ÄÊü•Êñ∞ÊñáÊú¨ÊòØÂê¶Â∑≤ÂåÖÂê´Âú®Á¥ØÁßØÊñáÊú¨‰∏≠ÔºåÊàñÁ¥ØÁßØÊñáÊú¨ÊòØÂê¶ÊòØÊñ∞ÊñáÊú¨ÁöÑ‰∏ÄÈÉ®ÂàÜ
            if (currentAccumulated && text) {
                if (text.includes(currentAccumulated)) {
                    // Êñ∞ÊñáÊú¨ÂåÖÂê´ÊóßÊñáÊú¨Ôºå‰ΩøÁî®Êñ∞ÊñáÊú¨
                    updatedText = text;
                } else if (!currentAccumulated.includes(text)) {
                    // Â¶ÇÊûúÊñ∞ÊñáÊú¨‰∏çÊòØÊóßÊñáÊú¨ÁöÑ‰∏ÄÈÉ®ÂàÜÔºå‰πü‰∏çÂåÖÂê´ÊóßÊñáÊú¨ÔºåÂàôÁ¥ØÂä†
                    // ÂèØËÉΩÊòØÊñ∞ÁöÑÂè•Â≠êÂºÄÂßã
                    updatedText = currentAccumulated + ' ' + text;
                } else {
                    // ÊóßÊñáÊú¨ÂåÖÂê´Êñ∞ÊñáÊú¨Ôºå‰øùÊåÅÊóßÊñáÊú¨
                    updatedText = currentAccumulated;
                }
            }

            // Êõ¥Êñ∞ÁïåÈù¢ÊòæÁ§∫ÂíåÁä∂ÊÄÅ
            this.setData({
                debugRecognizedText: updatedText,
                accumulatedText: updatedText
            });

            // ‰øùÂ≠òÊúÄÊñ∞ÊñáÊú¨Áî®‰∫éÂêéÁª≠ÂèëÈÄÅ
            this.latestRecognizedText = updatedText;

            // --- Silence Detection Logic ---
            // Clear any existing timer
            if (this.silenceTimer) {
                clearTimeout(this.silenceTimer);
                this.silenceTimer = null;
            }
            // If not final, and still recording, set a new timer
            // Don't set timer if result is final to avoid sending after manual stop
            if (!isFinal && this.data.isRecording) {
                this.silenceTimer = setTimeout(() => {
                    console.log('Silence detected, stopping recorder and sending to Deepseek...');
                    // Stop recording first, indicating it was triggered by silence
                    this.stopRecordingAndRecognition(true);
                    // Immediately attempt to send (sendToDeepseek will check text)
                    this.sendToDeepseek();
                }, 4000); // 4 seconds of silence
            }
            // If it *is* final (e.g., service decided end of speech),
            // and we have text, send it immediately? (Optional behavior)
            // For now, we only send on timer expiry.
        });

        // --- Error Callback ---
        this.speechRecognizer.onError((error: any) => {
            const now = Date.now();
            if (now - this.lastErrorTime < this.minErrorInterval) {
                console.warn("Duplicate error suppressed within interval:", error);
                return;
            }
            this.lastErrorTime = now;

            let processedError: any = error;
            let errorSource = "Unknown"; // For logging

            // Try to parse if it's a string containing JSON
            if (typeof error === 'string') {
                try {
                    processedError = JSON.parse(error);
                    errorSource = "Parsed String";
                } catch (e) {
                    console.warn("Error data is a non-JSON string:", error);
                    processedError = { message: error }; // Treat non-JSON string as a message
                    errorSource = "Raw String";
                }
            } else if (typeof error === 'object' && error !== null) {
                errorSource = "Object";
            }

            // Log the raw error object structure and content (using original error)
            // console.error(`ËØ≠Èü≥ËØÜÂà´ÈîôËØØ (Source: ${errorSource}, Raw): `, JSON.stringify(error, null, 2));
            // Log the processed error object
            // if (errorSource !== "Object") {
            //     console.error(`ËØ≠Èü≥ËØÜÂà´ÈîôËØØ (Processed): `, JSON.stringify(processedError, null, 2));
            // }

            // Ignore reset events for UI feedback
            if (processedError && processedError.code === "RESET") {
                console.log("Ignoring reset event in error handler.");
                return;
            }

            // ÂøΩÁï•Â∏∏ËßÅÁöÑÈùûÈîôËØØÁªàÊ≠¢Áä∂ÊÄÅ
            const ignoredErrors = [
                "END_OF_SPEECH",
                "SPEECH_RECOGNITION_STOPPED",
                "RECORDING_STOPPED",
                "RECOGNITION_TIMEOUT",
                "TASKFAILED", // Add TaskFailed (normalized to uppercase)
                "ERR_VOICE_RECOGNITION_TIMEOUT",
                "ERR_RECORDER_STOPPED"
            ];

            // Perform checks on the processedError
            const errorCode = processedError && processedError.code ? processedError.code : undefined;
            const errorName = processedError && processedError.header && processedError.header.name ? processedError.header.name.toUpperCase() : undefined;
            const errorMessage = processedError && processedError.message ? processedError.message.toUpperCase() : undefined;
            const errorErrMsg = processedError && processedError.errMsg ? processedError.errMsg.toUpperCase() : undefined;

            // Ê£ÄÊü•ÈîôËØØ‰ª£Á†Å„ÄÅÂêçÁß∞ÊàñÈîôËØØ‰ø°ÊÅØ‰∏≠ÊòØÂê¶ÂåÖÂê´ÂèØÂøΩÁï•ÁöÑÁä∂ÊÄÅ
            if (processedError && (
                (errorCode && ignoredErrors.includes(errorCode)) ||
                (errorName && ignoredErrors.includes(errorName)) ||
                (errorMessage && ignoredErrors.some(code => ignoredErrors.includes(code.toUpperCase()) && errorMessage.includes(code.toUpperCase()))) ||
                (errorErrMsg && ignoredErrors.some(code => ignoredErrors.includes(code.toUpperCase()) && errorErrMsg.includes(code.toUpperCase())))
            )) {
                console.log("Ê≠£Â∏∏ÁªìÊùüÁöÑËØ≠Èü≥ËØÜÂà´Áä∂ÊÄÅÊàñÂèØÂøΩÁï•ÁöÑTaskFailedÔºå‰∏çÈúÄË¶ÅÊòæÁ§∫ÈîôËØØ:", errorCode || errorName || errorMessage || errorErrMsg);
                return; // ‰∏çÊòæÁ§∫ÈîôËØØ‰ø°ÊÅØ
            }

            // If it reaches here, it's an unexpected error
            console.error('Êú™Â§ÑÁêÜÁöÑËØ≠Èü≥ËØÜÂà´ÈîôËØØÔºåÂç≥Â∞ÜÊòæÁ§∫ Toast:', processedError);

            // Handle other errors (use processedError for message extraction)
            wx.showToast({
                title: `ËØÜÂà´Âá∫Èîô: ${processedError.errMsg || processedError.message || 'Êú™Áü•ÈîôËØØ'}`,
                icon: 'none'
            });
            this.setData({
                isRecording: false,
                isWaitingForDeepseek: false,
                orbState: 'idle'
            });
            // Clear timer on error
            if (this.silenceTimer) {
                clearTimeout(this.silenceTimer);
                this.silenceTimer = null;
            }
            if (this.activityTimer) {
                clearTimeout(this.activityTimer);
                this.activityTimer = null;
            }
        });
    },

    // --- Handle Voice Activity ---
    // Called when we receive speech recognition results (indicating active speech)
    handleVoiceActivity: function () {
        // Update timestamp to track speech activity
        this.lastResultTime = Date.now();

        // Only change animation if we're in recording mode
        if (!this.data.isRecording) return;

        // Set to active bouncing state
        if (this.data.orbState !== 'listening-active') {
            this.setData({
                orbState: 'listening-active'
            });
        }

        // Clear any existing activity timer
        if (this.activityTimer) {
            clearTimeout(this.activityTimer);
        }

        // Set a timer to change back to normal listening state when activity stops
        this.activityTimer = setTimeout(() => {
            // Only reset if we're still in active state and recording
            if (this.data.orbState === 'listening-active' && this.data.isRecording) {
                this.setData({
                    orbState: 'listening'
                });
            }
            this.activityTimer = null;
        }, 800); // Return to normal listening state after 800ms without new results
    },

    // --- Show Text Input ---
    showTextInput: function () {
        // If speaking, stop the speech
        if (this.data.isSpeaking) {
            stopTTSPlayback(true); // Pass true to suppress error messages
            this.setData({
                isSpeaking: false,
                orbState: 'idle'
            });
        }

        // If recording, stop the recording
        if (this.data.isRecording) {
            this.stopRecordingAndRecognition();
        }

        this.setData({
            isEditing: true
        });
    },

    // --- Hide Text Input If Needed ---
    hideTextInputIfNeeded: function () {
        if (this.data.isEditing) {
            this.setData({
                isEditing: false
            });
        }
    },

    // --- Toggle Settings Menu ---
    toggleSettingsMenu: function () {
        const newState = !this.data.showSettings;
        console.log('[Settings] Toggling settings menu:', this.data.showSettings, '->', newState);
        this.setData({
            showSettings: newState
        });
    },

    // --- Hide Settings Menu ---
    hideSettingsMenu: function () {
        // Hide both text input and settings menu
        this.hideTextInputIfNeeded();

        if (this.data.showSettings) {
            this.setData({
                showSettings: false
            });
        }
    },

    // --- Prevent Event Bubbling ---
    preventBubble: function (e: WechatMiniprogram.TouchEvent) {
        // This prevents the container's tap event from firing
        // when tapping on the text input container or settings menu
        console.log('[UI] Preventing event bubble');
        // ÂæÆ‰ø°Â∞èÁ®ãÂ∫è‰∏≠‰ΩøÁî®catchtapÁªëÂÆöËÄå‰∏çÊòØ‰ΩøÁî®stopPropagation
        // Â∑≤Âú®wxml‰∏≠‰ΩøÁî®catchtap="preventBubble"
    },

    // --- Handle Text Input Change ---
    onTextInputChange: function (e: WechatMiniprogram.Input) {
        this.setData({
            debugRecognizedText: e.detail.value
        });
        this.latestRecognizedText = e.detail.value;
    },

    // --- Send Edited Text ---
    sendEditedText: function () {
        // Don't send if text is empty or waiting for response
        if (!this.data.debugRecognizedText.trim() || this.data.isWaitingForDeepseek) {
            return;
        }

        // Update latestRecognizedText before sending
        this.latestRecognizedText = this.data.debugRecognizedText;

        // Reset isEditing state
        this.setData({
            isEditing: false
        });

        // Send the message
        this.sendToDeepseek();
    },

    // --- Orb Tap Handler ---
    handleOrbTap: function () {
        // If editing, hide the input first
        if (this.data.isEditing) {
            this.setData({
                isEditing: false
            });
            return;
        }

        if (this.data.isRecording) {
            this.stopRecordingAndRecognition();
        } else if (this.data.isSpeaking) { // If speaking, tapping stops TTS and starts recording
            stopTTSPlayback(true); // Pass true to suppress error messages
            this.setData({ isSpeaking: false });
            // Immediately start recording after stopping TTS
            this.startRecordingAndRecognition();
        } else {
            this.startRecordingAndRecognition();
        }
    },

    // --- Start Recording ---
    startRecordingAndRecognition: function () {
        stopTTSPlayback(true); // Stop any ongoing TTS before starting recording, suppress errors
        if (this.data.isRecording || this.data.isWaitingForDeepseek) {
            return; // Already active or processing
        }
        console.log('ÂºÄÂßãÂΩïÈü≥ÂíåËØÜÂà´');

        // Reset state before starting
        if (this.speechRecognizer) {
            try {
                this.speechRecognizer.reset();
            } catch (e) {
                console.error('Reset error:', e);
            }
        }

        // Clear timers
        if (this.silenceTimer) {
            clearTimeout(this.silenceTimer);
            this.silenceTimer = null;
        }
        if (this.activityTimer) {
            clearTimeout(this.activityTimer);
            this.activityTimer = null;
        }

        this.setData({
            isRecording: true,
            isWaitingForDeepseek: false,
            orbState: 'listening', // Start with normal listening state
            debugRecognizedText: '', // Clear previous results
            debugDeepseekResponse: '',
            latestRecognizedText: '',
            accumulatedText: '', // ÈáçÁΩÆÁ¥ØÁßØÁöÑÊñáÊú¨
        });

        // Start the speech recognizer (which handles the recorder internally)
        if (this.speechRecognizer) {
            this.speechRecognizer.start().catch((error: any) => {
                console.error('ÂêØÂä®ËØ≠Èü≥ËØÜÂà´Â§±Ë¥•:', error);
                wx.showToast({
                    title: 'ÂêØÂä®ËØÜÂà´Â§±Ë¥•',
                    icon: 'none'
                });
                this.setData({
                    isRecording: false,
                    isWaitingForDeepseek: false,
                    orbState: 'idle'
                });
            });
        } else {
            console.error('ËØ≠Èü≥ËØÜÂà´Âô®Êú™ÂàùÂßãÂåñ');
            wx.showToast({
                title: 'ËØÜÂà´ÂäüËÉΩÂàùÂßãÂåñÂ§±Ë¥•',
                icon: 'none'
            });
            this.setData({ isRecording: false, orbState: 'idle' });
        }
    },

    // --- Stop Recording (Manual / Silence) ---
    stopRecordingAndRecognition: function (triggeredBySilence: boolean = false) { // Add parameter to know the context
        console.log(`ÂÅúÊ≠¢ÂΩïÈü≥ÂíåËØÜÂà´ (${triggeredBySilence ? 'ÈùôÈªòËß¶Âèë' : 'ÊâãÂä®'})`);
        if (!this.data.isRecording) return;

        // Clear timers
        if (this.silenceTimer) {
            clearTimeout(this.silenceTimer);
            this.silenceTimer = null;
        }
        if (this.activityTimer) {
            clearTimeout(this.activityTimer);
            this.activityTimer = null;
        }

        // Á°Æ‰øùÂú®ÂÅúÊ≠¢ÂΩïÈü≥Êó∂‰øùÁïôÁ¥ØÁßØÁöÑÊñáÊú¨
        if (this.data.accumulatedText) {
            this.latestRecognizedText = this.data.accumulatedText;
        }

        // Stop the recognizer (which stops the recorder)
        if (this.speechRecognizer) {
            this.speechRecognizer.stop().catch((err: any) => {
                console.error("Error stopping speech recognizer:", err);
                // Handle potential errors during stop if needed
            });
        }

        // ONLY update isRecording state here. Let the caller decide the next state.
        this.setData({
            isRecording: false
        });
    },

    // --- Handle Debug Text Input ---
    onDebugTextInput: function (e: WechatMiniprogram.Input) {
        // Êõ¥Êñ∞ debugRecognizedTextÔºåÂêåÊó∂Êõ¥Êñ∞Êàë‰ª¨Áî®‰∫éÂèëÈÄÅÁöÑÊñáÊú¨
        this.setData({
            debugRecognizedText: e.detail.value
        });
        this.latestRecognizedText = e.detail.value;
    },

    // --- Send Recognized Text to Deepseek ---
    sendToDeepseek: function () {
        if (!this.latestRecognizedText.trim() || this.data.isWaitingForDeepseek) {
            console.log('ÂèëÈÄÅÂèñÊ∂àÔºöÊó†ÊúâÊïàÊñáÊú¨ÊàñÂ∑≤Âú®Á≠âÂæÖÂõûÂ§ç');
            if (!this.data.isWaitingForDeepseek) {
                this.setData({ isRecording: false, orbState: 'idle', isWaitingForDeepseek: false });
            }
            return;
        }

        stopTTSPlayback(true); // Suppress errors since this is an intentional stop

        const userMessage: ChatMessage = {
            id: Date.now(),
            role: 'user',
            content: `"${this.latestRecognizedText}"`
        };

        this.setData({
            isRecording: false,
            isWaitingForDeepseek: true,
            isSpeaking: false,
            orbState: 'processing',
            chatHistory: [...this.data.chatHistory, userMessage],
            lastMessageId: `msg-${userMessage.id}`,
            debugRecognizedText: '', // Clear debug text once added to chat history
            accumulatedText: '', // Ê∏ÖÈô§Á¥ØÁßØÁöÑÊñáÊú¨
            isEditing: false,         // Ensure editing state is reset
        }, () => {
            // Scroll to the bottom after adding user message
            this.scrollToBottom();
        });

        // If in debug mode, return a mock response instead of making an API call
        if (this.data.isDebugMode) {
            console.log('[Debug] Simulating API response in debug mode');
            setTimeout(() => {
                const debugResponse = `[DEBUG MODE] You said: "${this.latestRecognizedText}"`;

                const assistantMessage: ChatMessage = {
                    id: Date.now(),
                    role: 'assistant',
                    content: debugResponse
                };

                this.setData({
                    chatHistory: [...this.data.chatHistory, assistantMessage],
                    lastMessageId: `msg-${assistantMessage.id}`,
                    debugDeepseekResponse: '',
                    isWaitingForDeepseek: false,
                    orbState: 'idle'
                }, () => {
                    this.scrollToBottom();
                });
            }, 1000);
            return;
        }

        // Initialize conversationHistory if it doesn't exist
        if (!this.conversationHistory) {
            this.conversationHistory = [];
        }

        // Add user's message to conversation history
        this.conversationHistory.push({
            role: 'user',
            content: this.latestRecognizedText
        });

        if (this.data.currentAiModel === 'deepseek') {
            // Original Deepseek implementation with chat history update
            wx.request({
                url: DEEPSEEK_SECRETS.API_URL,
                method: 'POST',
                header: {
                    'Content-Type': 'application/json',
                    'Authorization': `Bearer ${DEEPSEEK_SECRETS.API_KEY}`
                },
                data: {
                    model: DEEPSEEK_SECRETS.MODEL,
                    messages: [
                        { role: 'user', content: this.latestRecognizedText }
                    ],
                    temperature: 0.7,
                    max_tokens: 1000,
                    stream: true
                },
                responseType: 'text',
                success: (res: any) => {
                    console.log("Deepseek API ÊµÅÂºèÂìçÂ∫îÂºÄÂßã");
                    if (res.statusCode === 200 && res.data) {
                        let fullContent = '';
                        let sentenceEndDetected = false;
                        let firstChunkReceived = false;

                        // We need a way to process the stream chunk by chunk as it arrives.
                        // wx.request 'success' gives the whole data at once even with stream:true.
                        // For true streaming UI update & TTS, we'd need wx.requestTask.onChunkReceived (not standard)
                        // or a WebSocket connection to a backend proxy that handles the stream.

                        // --- SIMPLIFIED APPROACH for now: Process whole response, then TTS --- 
                        try {
                            const lines = res.data.split('\n');
                            for (const line of lines) {
                                if (!line.trim() || !line.startsWith('data:')) continue;
                                const jsonStr = line.substring(5).trim();
                                if (jsonStr === '[DONE]') continue;
                                try {
                                    const chunk = JSON.parse(jsonStr);

                                    // ÊèêÂèñÂ¢ûÈáèÂÜÖÂÆπ (Replace optional chaining)
                                    if (chunk.choices &&
                                        chunk.choices.length > 0 &&
                                        chunk.choices[0].delta &&
                                        chunk.choices[0].delta.content) {
                                        const delta = chunk.choices[0].delta;
                                        fullContent += delta.content;
                                    }
                                } catch (e) { /* ignore json parse error */ }
                            }

                            // Check for function triggers and buttons in the response
                            const { processedText, functionFound, buttons, shouldAutoRecord } = checkAndHandleFunctionTriggers(fullContent);

                            const assistantMessage: ChatMessage = {
                                id: Date.now(),
                                role: 'assistant',
                                content: processedText, // Use the processed text without function tags
                                buttons: buttons.length > 0 ? buttons : undefined, // Add buttons if any
                                hint: shouldAutoRecord ? '(Á≠âÂæÖÊÇ®ÁöÑËØ≠Èü≥ÂõûÂ§ç)' : undefined // Add a hint when auto-record is enabled
                            };

                            this.setData({
                                chatHistory: [...this.data.chatHistory, assistantMessage],
                                lastMessageId: `msg-${assistantMessage.id}`,
                                debugDeepseekResponse: '', // Clear after adding to chat history
                                isWaitingForDeepseek: false
                            }, () => {
                                // Scroll to the bottom after updating chat
                                this.scrollToBottom();
                            });

                            // --- Play the full response using TTS ---
                            if (processedText.trim()) {
                                playTextWithTTS.call(this, processedText, shouldAutoRecord, "#1");
                            } else {
                                // No content to speak, return to idle
                                this.setData({ orbState: 'idle' });
                            }

                        } catch (error) {
                            console.error('Â§ÑÁêÜÊµÅÂºèÂìçÂ∫îÂ§±Ë¥•:', error);
                            this.handleApiError('Â§ÑÁêÜÂìçÂ∫îÂ§±Ë¥•');
                        }
                    } else {
                        this.handleApiError(`ËØ∑Ê±ÇÂ§±Ë¥• (${res.statusCode})`);
                    }
                },
                fail: (error) => {
                    console.error('Deepseek API ËØ∑Ê±ÇÂ§±Ë¥•:', error);
                    this.handleApiError('ÁΩëÁªúËØ∑Ê±ÇÂ§±Ë¥•');
                }
            });
        } else {
            // Updated Qwen implementation with chat history
            // Create a copy of the conversation history for the API call
            const messages = [
                { role: 'system' as const, content: 'You are a helpful assistant.' },
                ...this.conversationHistory
            ];

            callQwenApiWithHistory(messages)
                .then(response => {
                    if (!response) {
                        throw new Error('Empty response from Qwen API');
                    }

                    // Add assistant response to conversation history
                    this.conversationHistory.push({
                        role: 'assistant',
                        content: response
                    });

                    // Check for function triggers and buttons in the response
                    const { processedText, functionFound, buttons, shouldAutoRecord } = checkAndHandleFunctionTriggers(response);
                    console.log('Initial prompt response with buttons:', buttons.length > 0);

                    const assistantMessage: ChatMessage = {
                        id: Date.now(),
                        role: 'assistant',
                        content: processedText, // Use processed text without function tags
                        buttons: buttons.length > 0 ? buttons : undefined, // Add buttons if any
                        hint: shouldAutoRecord ? '(Á≠âÂæÖÊÇ®ÁöÑËØ≠Èü≥ÂõûÂ§ç)' : undefined // Add a hint when auto-record is enabled
                    };

                    this.setData({
                        chatHistory: [...this.data.chatHistory, assistantMessage],
                        lastMessageId: `msg-${assistantMessage.id}`,
                        debugDeepseekResponse: '',
                        isWaitingForDeepseek: false
                    }, () => {
                        // Scroll to the bottom after updating chat
                        this.scrollToBottom();
                    });

                    if (processedText.trim()) {
                        console.log("[TTS] Attempting to play response: #2", processedText);
                        this.setData({ isSpeaking: true, orbState: 'speaking' });
                        playTextWithTTS.call(this, processedText, shouldAutoRecord, "#2");
                    } else {
                        this.setData({ orbState: 'idle' });
                    }
                })
                .catch(error => {
                    console.error('Qwen API request failed:', error);
                    this.handleApiError(error.message || 'ÁΩëÁªúËØ∑Ê±ÇÂ§±Ë¥•');
                });
        }
    },

    // --- API Error Handling ---
    handleApiError: function (errorMsg: string) {
        this.setData({
            debugDeepseekResponse: `ÈîôËØØ: ${errorMsg}`,
            isLoading: false, // Ensure isLoading is also reset if used elsewhere
            isWaitingForDeepseek: false,
            orbState: 'idle' // Reset orb state on error
        });
        wx.showToast({
            title: errorMsg,
            icon: 'none',
            duration: 2000
        });
    },

    // Helper method to ensure chat scrolls to the bottom
    scrollToBottom: function () {
        // Give the DOM time to update before scrolling
        setTimeout(() => {
            // Try to directly scroll to bottom first
            wx.createSelectorQuery()
                .select('.chat-history')
                .node()
                .exec((res) => {
                    const scrollView = res[0] && res[0].node;
                    if (scrollView) {
                        // Direct scrollTo method
                        scrollView.scrollTo({
                            top: 999999, // Very large number to ensure scroll to bottom
                            behavior: 'smooth'
                        });
                    } else {
                        // Fallback: Try to scroll to the element with the lastMessageId
                        if (this.data.lastMessageId) {
                            wx.pageScrollTo({
                                selector: `#${this.data.lastMessageId}`,
                                duration: 300
                            });
                        } else {
                            // Final fallback: just scroll to bottom of page
                            wx.pageScrollTo({
                                scrollTop: 9999,
                                duration: 300
                            });
                        }
                    }
                });
        }, 100); // Short delay to ensure DOM has updated
    },

    // --- Add method to switch engine (Example) ---
    switchAsrEngine: function () {
        const nextEngine: AsrEngineType = this.data.currentAsrEngine === 'alibaba' ? 'dashscope' : 'alibaba';
        console.log(`Switching ASR engine to: ${nextEngine}`);
        // Stop current recognizer if active
        if (this.speechRecognizer && this.data.isRecording) {
            this.speechRecognizer.stop();
        }
        if (this.speechRecognizer && typeof this.speechRecognizer.reset === 'function') {
            this.speechRecognizer.reset(); // Call reset if available
        }
        this.speechRecognizer = null; // Clear instance
        stopTTSPlayback(); // Stop TTS during switch
        this.setData({
            currentAsrEngine: nextEngine,
            isRecording: false,
            isWaitingForDeepseek: false,
            isSpeaking: false,
            orbState: 'idle',
            debugRecognizedText: '',
            debugDeepseekResponse: ''
        }, () => {
            this.initSpeechRecognition(); // Re-initialize with the new engine
            wx.showToast({ title: `Â∑≤ÂàáÊç¢Âà∞ ${nextEngine} ÂºïÊìé`, icon: 'none' });
        });
    },

    // Add new method to switch AI models
    switchAiModel: function () {
        const nextModel = this.data.currentAiModel === 'deepseek' ? 'qwen' : 'deepseek';
        this.setData({
            currentAiModel: nextModel,
            debugDeepseekResponse: '', // Clear previous response
            debugRecognizedText: '' // Clear previous input
        });
        wx.showToast({
            title: `Â∑≤ÂàáÊç¢Âà∞ ${nextModel} Ê®°Âûã`,
            icon: 'none'
        });
    },

    // Update clearChatHistory to also clear the conversation history
    clearChatHistory: function () {
        this.setData({
            chatHistory: [],
            lastMessageId: '',
            debugRecognizedText: '',
            debugDeepseekResponse: ''
        });

        // Clear the API conversation history as well
        this.conversationHistory = [];
    },

    // Add method to send initial prompt to AI
    sendInitialPromptToAI: function () {
        // Skip if there's already chat history (e.g. from a reload)
        if (this.data.chatHistory.length > 0) {
            return;
        }

        // Set initial prompt as if it came from user, but make it invisible
        // We're just sending it to the API, not showing it to user
        this.latestRecognizedText = AI_INITIAL_PROMPT;

        // Call the AI but don't add this prompt to visible chat history
        if (this.data.currentAiModel === 'deepseek') {
            this.callDeepseekWithoutUserMessage(AI_INITIAL_PROMPT);
        } else {
            this.callQwenWithoutUserMessage(AI_INITIAL_PROMPT);
        }
    },

    // Modified version of sendToDeepseek that doesn't add a user message
    callDeepseekWithoutUserMessage: function (prompt: string) {
        // Skip API call if in debug mode
        if (this.data.isDebugMode) {
            console.log('[Debug] Skipping Deepseek API call in debug mode');

            const debugMessage: ChatMessage = {
                id: Date.now(),
                role: 'assistant',
                content: 'üêû Debug mode active. This would be an API response from Deepseek.'
            };

            this.setData({
                chatHistory: [debugMessage],
                isWaitingForDeepseek: false,
                orbState: 'idle'
            });
            return;
        }

        // Continue with regular API call
        this.setData({
            isWaitingForDeepseek: true,
            orbState: 'processing'
        });

        // Initialize conversationHistory if it doesn't exist
        if (!this.conversationHistory) {
            this.conversationHistory = [];
        }

        wx.request({
            url: DEEPSEEK_SECRETS.API_URL,
            method: 'POST',
            header: {
                'Content-Type': 'application/json',
                'Authorization': `Bearer ${DEEPSEEK_SECRETS.API_KEY}`
            },
            data: {
                model: DEEPSEEK_SECRETS.MODEL,
                messages: [
                    { role: 'user', content: prompt }
                ],
                temperature: 0.7,
                max_tokens: 1000,
                stream: true
            },
            responseType: 'text',
            success: (res: any) => {
                console.log("Initial Deepseek API response received");
                if (res.statusCode === 200 && res.data) {
                    let fullContent = '';

                    try {
                        const lines = res.data.split('\n');
                        for (const line of lines) {
                            if (!line.trim() || !line.startsWith('data:')) continue;
                            const jsonStr = line.substring(5).trim();
                            if (jsonStr === '[DONE]') continue;
                            try {
                                const chunk = JSON.parse(jsonStr);

                                if (chunk.choices &&
                                    chunk.choices.length > 0 &&
                                    chunk.choices[0].delta &&
                                    chunk.choices[0].delta.content) {
                                    const delta = chunk.choices[0].delta;
                                    fullContent += delta.content;
                                }
                            } catch (e) { /* ignore json parse error */ }
                        }

                        // For initial prompt, extract the text, buttons, but skip function navigation
                        const { processedText, buttons, shouldAutoRecord } = checkAndHandleFunctionTriggers(fullContent);
                        console.log('Initial prompt response with buttons:', buttons.length > 0);

                        // Add the initial greeting from AI to chat with any buttons found
                        const assistantMessage: ChatMessage = {
                            id: Date.now(),
                            role: 'assistant',
                            content: processedText,
                            buttons: buttons.length > 0 ? buttons : undefined,
                            hint: shouldAutoRecord ? '(Á≠âÂæÖÊÇ®ÁöÑËØ≠Èü≥ÂõûÂ§ç)' : undefined
                        };

                        this.setData({
                            chatHistory: [assistantMessage], // For initial message, just set one message
                            lastMessageId: `msg-${assistantMessage.id}`,
                            debugDeepseekResponse: '',
                            isWaitingForDeepseek: false,
                            orbState: 'idle'
                        }, () => {
                            // Scroll to the bottom after updating chat
                            this.scrollToBottom();
                        });

                        // Optionally speak the welcome message
                        if (processedText.trim()) {
                            this.setData({ isSpeaking: true, orbState: 'speaking' });
                            console.log("[TTS] Attempting to play response: #3", processedText);
                            playTextWithTTS.call(this, processedText, shouldAutoRecord, "#3");
                        } else {
                            this.setData({ orbState: 'idle' });
                        }
                    } catch (error) {
                        console.error('Â§ÑÁêÜÂàùÂßãÂìçÂ∫îÂ§±Ë¥•:', error);
                        this.handleApiError('Â§ÑÁêÜÂìçÂ∫îÂ§±Ë¥•');
                    }
                } else {
                    this.handleApiError(`ËØ∑Ê±ÇÂ§±Ë¥• (${res.statusCode})`);
                }
            },
            fail: (error) => {
                console.error('ÂàùÂßã Deepseek API ËØ∑Ê±ÇÂ§±Ë¥•:', error);
                this.handleApiError('ÁΩëÁªúËØ∑Ê±ÇÂ§±Ë¥•');
            }
        });
    },

    // Modified version for Qwen API
    callQwenWithoutUserMessage: function (prompt: string) {
        // Skip API call if in debug mode
        if (this.data.isDebugMode) {
            console.log('[Debug] Skipping Qwen API call in debug mode');

            const debugMessage: ChatMessage = {
                id: Date.now(),
                role: 'assistant',
                content: 'üêû Debug mode active. This would be an API response from Qwen.'
            };

            this.setData({
                chatHistory: [debugMessage],
                isWaitingForDeepseek: false,
                orbState: 'idle'
            });
            return;
        }

        // Continue with regular API call
        this.setData({
            isWaitingForDeepseek: true,
            orbState: 'processing'
        });

        // Initialize conversation history with the system prompt
        if (!this.conversationHistory) {
            this.conversationHistory = [];
        }

        this.conversationHistory = [
            { role: 'system', content: 'You are a helpful assistant.' },
            { role: 'user', content: prompt }
        ];

        // Call API with the initial conversation
        callQwenApiWithHistory(this.conversationHistory)
            .then(response => {
                if (!response) {
                    throw new Error('Empty response from Qwen API');
                }

                // Add the response to conversation history
                this.conversationHistory.push({
                    role: 'assistant',
                    content: response
                });

                // For initial prompt, extract the text, buttons, but skip function navigation
                const { processedText, buttons, shouldAutoRecord } = checkAndHandleFunctionTriggers(response);
                console.log('Initial prompt response with buttons:', buttons.length > 0);

                const assistantMessage: ChatMessage = {
                    id: Date.now(),
                    role: 'assistant',
                    content: processedText,
                    buttons: buttons.length > 0 ? buttons : undefined,
                    hint: shouldAutoRecord ? '(Á≠âÂæÖÊÇ®ÁöÑËØ≠Èü≥ÂõûÂ§ç)' : undefined
                };

                this.setData({
                    chatHistory: [...this.data.chatHistory, assistantMessage],
                    lastMessageId: `msg-${assistantMessage.id}`,
                    isWaitingForDeepseek: false
                });

                if (processedText.trim()) {
                    this.setData({ isSpeaking: true, orbState: 'speaking' });
                    console.log("[TTS] Attempting to play response: #4", processedText);
                    playTextWithTTS.call(this, processedText, shouldAutoRecord, "#4");
                } else {
                    this.setData({ orbState: 'idle' });
                }
            })
            .catch(error => {
                console.error('Initial Qwen API request failed:', error);
                this.handleApiError(error.message || 'ÁΩëÁªúËØ∑Ê±ÇÂ§±Ë¥•');
            });
    },

    // Add new method to switch TTS providers
    switchTtsProvider: function () {
        // Switch between available providers
        const nextProvider: TtsProviderType = this.data.currentTtsProvider === 'ali' ? 'cosyvoice' : 'ali';
        console.log(`Switching TTS provider to: ${nextProvider}`);

        // Stop any ongoing TTS playback
        stopTTSPlayback(true); // Pass true to suppress error messages

        // Update the provider in the TTS service
        setTtsProvider(nextProvider);

        // Update the UI state
        this.setData({
            currentTtsProvider: nextProvider,
            isSpeaking: false
        });

        wx.showToast({
            title: `Â∑≤ÂàáÊç¢Âà∞ ${nextProvider === 'ali' ? 'ÈòøÈáå‰∫ë' : 'CosyVoice'} TTSÂºïÊìé`,
            icon: 'none',
            duration: 2000
        });
    },

    // Add method to toggle debug mode
    toggleDebugMode: function () {
        const newDebugMode = !this.data.isDebugMode;
        console.log(`[Debug] Toggling debug mode: ${this.data.isDebugMode} -> ${newDebugMode}`);

        // If turning off debug mode
        if (!newDebugMode) {
            // First clear chat history to allow the initial prompt to be sent
            this.setData({
                isDebugMode: newDebugMode,
                chatHistory: []  // Clear the chat history
            });

            // Clear the API conversation history as well
            if (this.conversationHistory) {
                this.conversationHistory = [];
            }

            console.log('[Debug] Debug mode turned off, sending initial prompt to AI');
            this.sendInitialPromptToAI();
        } else {
            // ÂàáÊç¢Âà∞ Debug Ê®°ÂºèÔºåÊòæÁ§∫Ëá™ÂÆö‰πâÈóÆÂÄôËØ≠
            // Ëá™ÂÆö‰πâÁöÑ Debug Ê®°ÂºèÈóÆÂÄôËØ≠ÔºåÂåÖÂê´ÊåâÈíÆÂíåËÆ∞ÂΩïÊ†áÁ≠æ
            const debugWelcomeText = "Debug: ÊÇ®Â•ΩÂëÄÔºåÊúâ‰ªÄ‰πàÊàëÂèØ‰ª•Â∏ÆÊÇ®ÁöÑÂêóÔºü[button:hongbao] [button:health] [button:emergency] [record]";

            // Â§ÑÁêÜÊñáÊú¨‰∏≠ÁöÑÊ†áÁ≠æÔºåÊèêÂèñÊåâÈíÆÂíåÂÖ∂‰ªñÂäüËÉΩ
            const { processedText, functionFound, buttons, shouldAutoRecord } = checkAndHandleFunctionTriggers(debugWelcomeText);

            // ÂàõÂª∫ÁªèËøáÂ§ÑÁêÜÁöÑË∞ÉËØïÊ∂àÊÅØ
            const debugMessage: ChatMessage = {
                id: Date.now(),
                role: 'assistant',
                content: processedText,
                buttons: buttons.length > 0 ? buttons : undefined,
                hint: shouldAutoRecord ? '(Á≠âÂæÖÊÇ®ÁöÑËØ≠Èü≥ÂõûÂ§ç)' : undefined
            };

            // Just update the debug mode flag and set the debug welcome message
            this.setData({
                isDebugMode: newDebugMode,
                chatHistory: [debugMessage]
            });
        }

        wx.showToast({
            title: newDebugMode ? 'Debug mode enabled' : 'Debug mode disabled',
            icon: 'none'
        });
    },

    // Removed: sendMessage, onMessageInput, scrollToBottom, saveChatHistory, loadChatHistory, clearChatHistory, activateDeepThinking, activateWebSearch, navigateToNewPage, initRecorderManager (now implicit)
});
